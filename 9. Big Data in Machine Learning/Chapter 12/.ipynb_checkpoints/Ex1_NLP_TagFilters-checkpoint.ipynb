{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Spark Standalone Cluster\n",
    "## Ex1: NLP - Tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement: Build a tags filter. Use the various NLP tools and a classifier, to predict tag for one question.  In future questions could be auto-tagged by such a classifier or tags could be recommended to users prior to posting.\n",
    "- Dataset: stack-overflow-data.csv. It contains Stack Overflow questions and associated tags.\n",
    "- Link tham kháº£o: http://benalexkeen.com/multiclass-text-classification-with-pyspark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link HDFS: http://172.24.40.251:50070/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '6g')\n",
    "\n",
    "sc = SparkContext(master='spark://172.25.53.2:7077', appName='Stack_Overflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://PM503-GV:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://172.25.53.2:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Stack_Overflow</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://172.25.53.2:7077 appName=Stack_Overflow>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"hdfs://172.24.40.251:19000/stack_overflow_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(file_name, inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                post|       tags|\n",
      "+--------------------+-----------+\n",
      "|what is causing t...|         c#|\n",
      "|have dynamic html...|    asp.net|\n",
      "|how to convert a ...|objective-c|\n",
      "|.net framework 4 ...|       .net|\n",
      "|trying to calcula...|     python|\n",
      "+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         tags|count|\n",
      "+-------------+-----+\n",
      "|       iphone| 2000|\n",
      "|      android| 2000|\n",
      "|           c#| 2000|\n",
      "|         null|20798|\n",
      "|      asp.net| 2000|\n",
      "|         html| 2000|\n",
      "|        mysql| 2000|\n",
      "|       jquery| 2000|\n",
      "|   javascript| 2000|\n",
      "|          css| 2000|\n",
      "|          sql| 2000|\n",
      "|          c++| 2000|\n",
      "|            c| 2000|\n",
      "|  objective-c| 2000|\n",
      "|         java| 2000|\n",
      "|          php| 2000|\n",
      "|         .net| 2000|\n",
      "|          ios| 2000|\n",
      "|       python| 2000|\n",
      "|    angularjs| 2000|\n",
      "|ruby-on-rails| 2000|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupby('tags').count().show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_null_data = data.filter(data.tags.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20798"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_null_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter(data.tags.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a new length feature: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('length',length(data['post']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------+\n",
      "|                post|         tags|length|\n",
      "+--------------------+-------------+------+\n",
      "|what is causing t...|           c#|   833|\n",
      "|have dynamic html...|      asp.net|   804|\n",
      "|how to convert a ...|  objective-c|   755|\n",
      "|.net framework 4 ...|         .net|   349|\n",
      "|trying to calcula...|       python|  1290|\n",
      "|how to give alias...|      asp.net|   309|\n",
      "|window.open() ret...|    angularjs|   495|\n",
      "|identifying serve...|       iphone|   424|\n",
      "|unknown method ke...|ruby-on-rails|  2022|\n",
      "|from the include ...|    angularjs|  1279|\n",
      "|when we need inte...|           c#|   995|\n",
      "|how to install .i...|          ios|   344|\n",
      "|dynamic textbox t...|      asp.net|   389|\n",
      "|rather than bubbl...|            c|  1338|\n",
      "|site deployed in ...|      asp.net|   349|\n",
      "|connection in .ne...|         .net|   228|\n",
      "|how to subtract 1...|  objective-c|    62|\n",
      "|ror console show ...|ruby-on-rails|  2594|\n",
      "|distance between ...|       iphone|   336|\n",
      "|sql query - how t...|          sql|  1037|\n",
      "+--------------------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|         tags|avg(length)|\n",
      "+-------------+-----------+\n",
      "|       iphone|    709.621|\n",
      "|      android|  1713.4345|\n",
      "|           c#|  1145.3065|\n",
      "|      asp.net|     999.95|\n",
      "|         html|   891.3105|\n",
      "|        mysql|   1038.561|\n",
      "|       jquery|   1081.507|\n",
      "|   javascript|    964.396|\n",
      "|          css|    954.809|\n",
      "|          sql|    870.912|\n",
      "|          c++|   1295.955|\n",
      "|            c|  1121.1115|\n",
      "|  objective-c|   972.8925|\n",
      "|         java|   1357.308|\n",
      "|          php|  1123.4205|\n",
      "|         .net|   731.0075|\n",
      "|          ios|   970.7565|\n",
      "|       python|  1018.6695|\n",
      "|    angularjs|  1294.7545|\n",
      "|ruby-on-rails|  1244.2055|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pretty Clear Difference\n",
    "data.groupby('tags').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BsTextExtractor(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(BsTextExtractor, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "\n",
    "        def f(s):\n",
    "            cleaned_post = BeautifulSoup(s).text\n",
    "            return cleaned_post\n",
    "\n",
    "        t = StringType()\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer\n",
    "text_extractor = BsTextExtractor(inputCol=\"post\", outputCol=\"cleaned_post\")\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_post\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n",
    "idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n",
    "class_to_num = StringIndexer(inputCol='tags',outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "We'll use Naive Bayes, but feel free to play around with this choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use defaults\n",
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipe = Pipeline(stages=[class_to_num,text_extractor,tokenizer,stopremove,count_vec,idf,clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = data_prep_pipe.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.select(['label','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "| 14.0|(262145,[0,1,2,3,...|\n",
      "|  3.0|(262145,[0,12,31,...|\n",
      "| 16.0|(262145,[0,1,2,3,...|\n",
      "|  8.0|(262145,[0,18,21,...|\n",
      "|  9.0|(262145,[0,1,4,8,...|\n",
      "|  3.0|(262145,[0,12,21,...|\n",
      "| 11.0|(262145,[0,1,3,6,...|\n",
      "| 18.0|(262145,[0,44,61,...|\n",
      "|  6.0|(262145,[0,1,14,2...|\n",
      "| 11.0|(262145,[0,1,3,4,...|\n",
      "| 14.0|(262145,[0,2,3,6,...|\n",
      "|  1.0|(262145,[0,18,27,...|\n",
      "|  3.0|(262145,[0,7,12,1...|\n",
      "| 13.0|(262145,[0,1,2,3,...|\n",
      "|  3.0|(262145,[0,11,27,...|\n",
      "|  8.0|(262145,[0,187,23...|\n",
      "| 16.0|(262145,[0,10,15,...|\n",
      "|  6.0|(262145,[0,1,3,12...|\n",
      "| 18.0|(262145,[0,30,39,...|\n",
      "|  0.0|(262145,[0,12,15,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training,testing) = clean_data.randomSplit([0.7,0.3], seed=142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = predictor.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(262145,[0,1,4,11...|[-21169.668465935...|[1.21437232270286...|      12.0|\n",
      "|  0.0|(262145,[0,1,4,14...|[-1167.4513441585...|[1.0,7.6094842671...|       0.0|\n",
      "|  0.0|(262145,[0,1,5,14...|[-11564.690744964...|[1.0,0.0,0.0,0.0,...|       0.0|\n",
      "|  0.0|(262145,[0,1,5,14...|[-10439.407817798...|[1.93390066856389...|      12.0|\n",
      "|  0.0|(262145,[0,1,7,9,...|[-7336.5344958838...|[1.0,1.5922509763...|       0.0|\n",
      "|  0.0|(262145,[0,1,7,11...|[-8486.4019708681...|[1.0,2.0825090588...|       0.0|\n",
      "|  0.0|(262145,[0,1,9,10...|[-3738.5573599120...|[7.30286711565549...|      12.0|\n",
      "|  0.0|(262145,[0,1,9,12...|[-6617.1798543878...|[0.99995740820888...|       0.0|\n",
      "|  0.0|(262145,[0,1,9,12...|[-1693.4933675940...|[2.32251400846856...|      12.0|\n",
      "|  0.0|(262145,[0,1,10,1...|[-3989.2579384985...|[1.0,3.3928388326...|       0.0|\n",
      "|  0.0|(262145,[0,1,10,1...|[-2373.9652840133...|[1.0,2.7652251546...|       0.0|\n",
      "|  0.0|(262145,[0,1,11,1...|[-8818.6290325138...|[1.0,1.4580653626...|       0.0|\n",
      "|  0.0|(262145,[0,1,11,1...|[-4285.8899650084...|[1.0,9.2886878940...|       0.0|\n",
      "|  0.0|(262145,[0,1,11,1...|[-2804.2071232226...|[1.0,3.5487447155...|       0.0|\n",
      "|  0.0|(262145,[0,1,11,1...|[-2115.4985343601...|[6.48717059350829...|       4.0|\n",
      "|  0.0|(262145,[0,1,12,1...|[-2828.9699626115...|[0.13219295903883...|      12.0|\n",
      "|  0.0|(262145,[0,1,12,1...|[-5997.8626004770...|[1.0,1.4086114099...|       0.0|\n",
      "|  0.0|(262145,[0,1,12,1...|[-3875.2807055205...|[1.0,2.0818433425...|       0.0|\n",
      "|  0.0|(262145,[0,1,12,1...|[-5370.3720000707...|[1.0,2.0080477642...|       0.0|\n",
      "|  0.0|(262145,[0,1,12,1...|[-7260.3672598605...|[1.0,1.6809702518...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  8.0|       3.0|   53|\n",
      "| 16.0|       8.0|   20|\n",
      "| 19.0|       5.0|    1|\n",
      "| 10.0|       1.0|    3|\n",
      "| 12.0|       5.0|    2|\n",
      "|  0.0|      12.0|  169|\n",
      "|  0.0|       8.0|   17|\n",
      "|  1.0|      19.0|    9|\n",
      "|  1.0|      12.0|    1|\n",
      "|  7.0|       3.0|   11|\n",
      "| 15.0|      11.0|   12|\n",
      "| 19.0|      12.0|    1|\n",
      "| 17.0|       7.0|   12|\n",
      "| 17.0|      19.0|   15|\n",
      "| 11.0|      17.0|    7|\n",
      "|  8.0|       6.0|    1|\n",
      "| 17.0|       9.0|    6|\n",
      "|  4.0|       6.0|    1|\n",
      "|  3.0|       9.0|    3|\n",
      "|  3.0|       5.0|    1|\n",
      "+-----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "test_results.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting: 0.7148108175153408\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save hdfs\n",
    "nb.save(\"hdfs://172.24.40.251:19000/NB_TagFilters_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not very good result! (~72%)\n",
    "- Solution: Try switching out the classification models! Or even try to come up with other engineered features!..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LogisticRegression/Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_1 = lg.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_1 = predictor_1.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  8.0|       3.0|   47|\n",
      "| 16.0|       8.0|   10|\n",
      "| 19.0|       5.0|    1|\n",
      "| 10.0|       1.0|    1|\n",
      "|  2.0|       0.0|    2|\n",
      "| 15.0|      16.0|    1|\n",
      "| 12.0|       5.0|    3|\n",
      "|  0.0|      12.0|   94|\n",
      "|  0.0|       8.0|    8|\n",
      "|  1.0|      19.0|   12|\n",
      "|  1.0|      12.0|    3|\n",
      "|  7.0|       3.0|    6|\n",
      "| 15.0|      11.0|    5|\n",
      "| 19.0|      12.0|    5|\n",
      "| 11.0|      17.0|   15|\n",
      "| 17.0|       7.0|   13|\n",
      "| 17.0|      19.0|    2|\n",
      "| 17.0|       9.0|   11|\n",
      "|  8.0|       6.0|    5|\n",
      "|  6.0|       1.0|    2|\n",
      "+-----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "test_results_1.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting: 0.7182544021412621\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc_1 = acc_eval.evaluate(test_results_1)\n",
    "print(\"Accuracy of model at predicting: {}\".format(acc_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It's not better result!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save hdfs\n",
    "lg.save(\"hdfs://172.24.40.251:19000/LG_TagFilters_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 500, \\\n",
    "                            maxDepth = 5, \\\n",
    "                            maxBins = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_2 = rf.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_2 = predictor_2.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  8.0|       3.0|   33|\n",
      "| 16.0|       8.0|   31|\n",
      "| 19.0|       5.0|    1|\n",
      "|  0.0|      12.0|  268|\n",
      "|  1.0|      19.0|    5|\n",
      "|  1.0|      12.0|   19|\n",
      "|  0.0|       8.0|    3|\n",
      "| 19.0|      12.0|   28|\n",
      "|  7.0|       3.0|    1|\n",
      "| 15.0|      11.0|    1|\n",
      "| 11.0|      17.0|    4|\n",
      "| 17.0|       7.0|   16|\n",
      "|  8.0|       6.0|    4|\n",
      "| 17.0|       9.0|    5|\n",
      "|  6.0|       1.0|    1|\n",
      "|  3.0|       9.0|    1|\n",
      "|  4.0|       6.0|    1|\n",
      "|  6.0|       8.0|    9|\n",
      "| 15.0|      19.0|    1|\n",
      "| 14.0|       7.0|    3|\n",
      "+-----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "test_results_2.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       8.0| 1046|\n",
      "|       0.0|  375|\n",
      "|       7.0|  582|\n",
      "|      18.0|  426|\n",
      "|       1.0|  524|\n",
      "|       4.0|  762|\n",
      "|      11.0|  580|\n",
      "|      14.0|  251|\n",
      "|       3.0|  491|\n",
      "|      19.0|  396|\n",
      "|       2.0|  647|\n",
      "|      17.0|  402|\n",
      "|      10.0|  624|\n",
      "|      13.0|  720|\n",
      "|       6.0|  578|\n",
      "|      15.0|  605|\n",
      "|       5.0|  520|\n",
      "|       9.0|  592|\n",
      "|      16.0|  656|\n",
      "|      12.0| 1113|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_2.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting: 0.7231428382621157\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc_2 = acc_eval.evaluate(test_results_2)\n",
    "print(\"Accuracy of model at predicting: {}\".format(acc_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It has higher accuracy but is not a better result!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save hdfs\n",
    "rf.save(\"hdfs://172.24.40.251:19000/RF_TagFilters_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
