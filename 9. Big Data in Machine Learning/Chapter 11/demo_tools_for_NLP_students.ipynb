{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhGbZVXumcwx"
   },
   "outputs": [],
   "source": [
    "# !apt update\n",
    "# !apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "# !wget -q http://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
    "# !tar -xvf spark-3.3.0-bin-hadoop3.tgz\n",
    "# !pip install -q findspark\n",
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lTSukfVFoM-4"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FPAqiuGwmqmA"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF, Tokenizer\n",
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XqwEY2jQmuHH"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrCbweEPp3Fo"
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CMQZenS8m6tI"
   },
   "outputs": [],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I know Spark can work well with NLP\"),\n",
    "    (2, \"Logistic,regression,models,are,supervised\")\n",
    "], [\"id\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-XDJXnJm-wX",
    "outputId": "79825416-3515-47ea-d90a-76e9d6971ea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------+\n",
      "|id |sentence                                 |\n",
      "+---+-----------------------------------------+\n",
      "|0  |Hi I heard about Spark                   |\n",
      "|1  |I know Spark can work well with NLP      |\n",
      "|2  |Logistic,regression,models,are,supervised|\n",
      "+---+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lejOkwfwp5-X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+--------------------------------------------+------+\n",
      "|sentence                                 |words                                       |tokens|\n",
      "+-----------------------------------------+--------------------------------------------+------+\n",
      "|Hi I heard about Spark                   |[hi, i, heard, about, spark]                |5     |\n",
      "|I know Spark can work well with NLP      |[i, know, spark, can, work, well, with, nlp]|8     |\n",
      "|Logistic,regression,models,are,supervised|[logistic,regression,models,are,supervised] |1     |\n",
      "+-----------------------------------------+--------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lTCrkb8Xp6F3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----------------------------------------------+------+\n",
      "|sentence                                 |words                                          |tokens|\n",
      "+-----------------------------------------+-----------------------------------------------+------+\n",
      "|Hi I heard about Spark                   |[hi, i, heard, about, spark]                   |5     |\n",
      "|I know Spark can work well with NLP      |[i, know, spark, can, work, well, with, nlp]   |8     |\n",
      "|Logistic,regression,models,are,supervised|[logistic, regression, models, are, supervised]|5     |\n",
      "+-----------------------------------------+-----------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ocv_yo0ep7Pv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNKjY8LEp7d_"
   },
   "source": [
    "## StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZGdhKEwqDWW"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, ['I', 'go', 'to', 'school', 'by', 'bus'])\n",
    "    (0, ['I', 'go', 'to', 'school', 'by', 'bus'])\n",
    "    [\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvG_c5plqZ7f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VD9waIcaqabv"
   },
   "source": [
    "## N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cCWDcz6HqbrW"
   },
   "outputs": [],
   "source": [
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"know\", \"Spark\", \"can\", \"work\", \"well\", \"with\", \"NLP\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"supervised\"])\n",
    "], [\"id\", \"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EZZ9zFsbqmjH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|ngrams                                                                   |\n",
      "+-------------------------------------------------------------------------+\n",
      "|[Hi I, I heard, heard about, about Spark]                                |\n",
      "|[I know, know Spark, Spark can, can work, work well, well with, with NLP]|\n",
      "|[Logistic regression, regression models, models are, are supervised]     |\n",
      "+-------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZR5_Os5rVW4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnXR96teqnrH"
   },
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OPYOfn_wqsh-"
   },
   "outputs": [],
   "source": [
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \")),\n",
    "    (2, \"a b d d a c c\".split(\" \"))\n",
    "], [\"id\", \"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "M3L9xbPrq0dO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+-------------------------------+\n",
      "|id |words                |features                       |\n",
      "+---+---------------------+-------------------------------+\n",
      "|0  |[a, b, c]            |(4,[0,1,2],[1.0,1.0,1.0])      |\n",
      "|1  |[a, b, b, c, a]      |(4,[0,1,2],[2.0,1.0,2.0])      |\n",
      "|2  |[a, b, d, d, a, c, c]|(4,[0,1,2,3],[2.0,2.0,1.0,2.0])|\n",
      "+---+---------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=4, minDF=1)\n",
    "\n",
    "model = cv.fit(df)\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75WUG1i6rSff"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2myQ1Qt1q3ce"
   },
   "source": [
    "## IF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tVvyFJCrq4_f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|label|sentence     |\n",
      "+-----+-------------+\n",
      "|0.0  |a b c        |\n",
      "|0.0  |a b c a      |\n",
      "|1.0  |a b d d a c c|\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"a b c\"),\n",
    "    (0.0, \"a b c a\"),\n",
    "    (1.0, \"a b d d a c c\")\n",
    "], [\"label\", \"sentence\"])\n",
    "sentenceData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ku8XMnGGrRXH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|label|sentence     |\n",
      "+-----+-------------+\n",
      "|0.0  |a b c        |\n",
      "|0.0  |a b c a      |\n",
      "|1.0  |a b d d a c c|\n",
      "+-----+-------------+\n",
      "\n",
      "+-----+-------------+---------------------+\n",
      "|label|sentence     |words                |\n",
      "+-----+-------------+---------------------+\n",
      "|0.0  |a b c        |[a, b, c]            |\n",
      "|0.0  |a b c a      |[a, b, c, a]         |\n",
      "|1.0  |a b d d a c c|[a, b, d, d, a, c, c]|\n",
      "+-----+-------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF, Tokenizer\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"a b c\"),\n",
    "    (0.0, \"a b c a\"),\n",
    "    (1.0, \"a b d d a c c\")\n",
    "], [\"label\", \"sentence\"])\n",
    "sentenceData.show(truncate=False)\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "wordsData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ltQBvkivrR8X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+---------------------+-------------------------------+\n",
      "|label|sentence     |words                |rawFeatures                    |\n",
      "+-----+-------------+---------------------+-------------------------------+\n",
      "|0.0  |a b c        |[a, b, c]            |(4,[0,1,2],[1.0,1.0,1.0])      |\n",
      "|0.0  |a b c a      |[a, b, c, a]         |(4,[0,1,2],[2.0,1.0,1.0])      |\n",
      "|1.0  |a b d d a c c|[a, b, d, d, a, c, c]|(4,[0,1,2,3],[2.0,2.0,1.0,2.0])|\n",
      "+-----+-------------+---------------------+-------------------------------+\n",
      "\n",
      "+-----+----------------------------------------------+\n",
      "|label|features                                      |\n",
      "+-----+----------------------------------------------+\n",
      "|0.0  |(4,[0,1,2],[0.0,0.0,0.0])                     |\n",
      "|0.0  |(4,[0,1,2],[0.0,0.0,0.0])                     |\n",
      "|1.0  |(4,[0,1,2,3],[0.0,0.0,0.0,1.3862943611198906])|\n",
      "+-----+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_TF = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize=4, minDF=1) # số thuộc tính tối đa sẽ lấy (số cột)\n",
    "model_cv_TF = cv_TF.fit(wordsData)\n",
    "featurizedData = model_cv_TF.transform(wordsData)\n",
    "featurizedData.show(truncate=False)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.select(\"label\", \"features\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
